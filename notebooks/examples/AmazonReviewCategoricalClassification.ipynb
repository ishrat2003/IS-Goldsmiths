{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "# Implementation of https://tm3.ghost.io/2017/04/21/amazon-food-reviews-part-vi/\n",
    "\n",
    "import time\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cleaning data\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#apply the polarity score\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "toPickle = False\n",
    "inputFilePath = \"/notebooks/data/amazon/\"\n",
    "outputFilePath = \"/notebooks/output/amazon/\"\n",
    "inputFileName = \"Review100.csv\"\n",
    "outputFileName = \"amz_data.pkl\"\n",
    "\n",
    "if toPickle:\n",
    "    filePath = inputFilePath + inputFileName\n",
    "    data = pd.read_csv(filePath)\n",
    "    print(data.dtypes)\n",
    "    print('------------------------------')\n",
    "    \n",
    "    data['text_cln']= data['Text'].map(lambda x: BeautifulSoup(x, \"lxml\").get_text())\n",
    "    print(data['text_cln'][0:5])\n",
    "    print('------------------------------')\n",
    "    \n",
    "    #apply the polarity score to each text feature using \n",
    "\n",
    "    data['tb_polarity']= data['text_cln'].map(lambda x: \n",
    "    TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    print(data['tb_polarity'][0:5])\n",
    "    print('------------------------------')\n",
    "    \n",
    "    #pickle\n",
    "    data.to_pickle(outputFilePath + outputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not toPickle:\n",
    "    data = pd.read_pickle(outputFilePath + outputFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#normalize date time\n",
    "data2 = data.copy()\n",
    "data2['datetime'] = data2['Time'].map(lambda x: (datetime.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S')))\n",
    "data2['datetime'] = pd.to_datetime(data2['datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk\n",
    "\n",
    "#train/test split 80/20\n",
    "X_data, y_target = data2['text_cln'], data2['Score']\n",
    "y_target = y_target.values\n",
    "X_datatrain, X_datatest, y_train, y_test = sk.train_test_split(X_data, y_target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.Thank you for the personal, incredible service!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(data2['text_cln'].iloc[10])\n",
    "print(data2['Score'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.018402814865112305 seconds ---\n",
      "--- 0.02248692512512207 seconds ---\n",
      "--- 0.022881269454956055 seconds ---\n",
      "Total words: 1560\n",
      "79 train sequences\n",
      "20 test sequences\n",
      "x_train shape: (79, 800)\n",
      "x_test shape: (20, 800)\n",
      "[[ 70 109   4 ...   0   0   0]\n",
      " [201 245 227 ...   0   0   0]\n",
      " [133   4   5 ...   0   0   0]\n",
      " ...\n",
      " [ 52  54   0 ...   0   0   0]\n",
      " [133   4   5 ...   0   0   0]\n",
      " [  0 259  90 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Process vocabulary  \n",
    "learn = tf.contrib.learn\n",
    "MAX_DOCUMENT_LENGTH = 800\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "start_time = time.time() #timing it \n",
    "\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_datatrain))) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "x_test = np.array(list(vocab_processor.transform(X_datatest)))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "print('Total words: %d' % n_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "#confirm that the shape is consistent with max_document_lenght = 800\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = MAX_DOCUMENT_LENGTH\n",
    "batch_size = 32\n",
    "#An epoch is a full pass over your training data\n",
    "#this one will pass over the training set 5 times\n",
    "#In 32 batches\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (79, 6)\n",
      "y_test shape: (20, 6)\n"
     ]
    }
   ],
   "source": [
    "#number of target classes\n",
    "num_classes = np.max(y_train)+1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "  '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.3277416229248047 seconds ---\n",
      "Train on 71 samples, validate on 8 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "71/71 [==============================] - 0s 5ms/sample - loss: 15.0942 - acc: 0.0563 - val_loss: 14.3049 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "71/71 [==============================] - 0s 449us/sample - loss: 13.5760 - acc: 0.1408 - val_loss: 13.3100 - val_acc: 0.1250\n",
      "Epoch 3/5\n",
      "71/71 [==============================] - 0s 701us/sample - loss: 12.4285 - acc: 0.2113 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 4/5\n",
      "71/71 [==============================] - 0s 609us/sample - loss: 12.7610 - acc: 0.1972 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 5/5\n",
      "71/71 [==============================] - 0s 580us/sample - loss: 12.2588 - acc: 0.2394 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "--- 1.516113519668579 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "#timed it at around 8 minutes\n",
    "history = model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_split=0.1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 264us/sample - loss: 11.4685 - acc: 0.2000\n",
      "--- 1.544198751449585 seconds ---\n",
      "Test score: 11.468515396118164\n",
      "Test accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                   batch_size=batch_size, verbose=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/sample\n",
      "20/20 [==============================] - 0s 63us/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.0872548e-33, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9709904e-01, 2.9009460e-03],\n",
       "       [0.0000000e+00, 9.7543567e-22, 0.0000000e+00, 1.0000000e+00,\n",
       "        3.3033477e-27, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.7725798e-25,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 2.7891183e-09],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.3140781e-24, 0.0000000e+00],\n",
       "       [0.0000000e+00, 3.2936886e-03, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9670631e-01, 0.0000000e+00],\n",
       "       [1.9945020e-12, 6.3718599e-01, 0.0000000e+00, 9.2906945e-29,\n",
       "        1.8552267e-04, 3.6262855e-01],\n",
       "       [0.0000000e+00, 4.4708108e-38, 0.0000000e+00, 0.0000000e+00,\n",
       "        6.8213222e-19, 1.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.5802681e-25, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.8747769e-38, 1.0000000e+00],\n",
       "       [4.5948548e-29, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 4.3964517e-28, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 8.0191190e-31],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.3106802e-06, 9.9999774e-01],\n",
       "       [0.0000000e+00, 2.2822086e-19, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 8.0214137e-16]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test, batch_size=batch_size, verbose=1)\n",
    "model.predict_proba(x_test, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
