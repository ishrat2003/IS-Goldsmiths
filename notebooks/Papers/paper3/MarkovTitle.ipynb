{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "import random\n",
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "class Markov(object):\n",
    "    def __init__(self, order=2, dictFile=\"\", maxWordInSentence=20, fileEncoding=\"utf-8\"):\n",
    "        self.table = {}\n",
    "        self.inputLineCount = 0\n",
    "        self.inputWordCount = 0\n",
    "        self.setOrder( order )\n",
    "        self.setMaxWordInSentence(maxWordInSentence)\n",
    "        if dictFile:\n",
    "            self.loadDictionary(dictFile, fileEncoding)\n",
    "\n",
    "\n",
    "    def setOrder(self, order=2):\n",
    "        self.order = order\n",
    "\n",
    "\tdef loadDictionary(self, dictFile, fileEncoding=\"utf-8\"):\n",
    "\t\tprint(\"Loaded dictionary file: \" + dictFile)\n",
    "\t\twith open(dictFile, 'r', encoding=fileEncoding) as inf:\n",
    "\t\t\tself.table = eval(inf.read())\n",
    "\n",
    "\t\t# pprint.pprint(self.table)\n",
    "\n",
    "\tdef readFile(self, filename, fileEncoding=\"utf-8\"):\n",
    "\t\twith  open(filename, \"r\", encoding=fileEncoding) as file:\n",
    "\t\t\tstrLine = \" \".join(file)\n",
    "\t\t\tself.processSection(strLine)\n",
    "\n",
    "\tdef processSection(self,line ):\n",
    "\t\t#print('---------------------------------')\n",
    "\t\t# global lineCount, wordCount, table, keyLen\n",
    "\t\tsent_text = nltk.sent_tokenize(line) # this gives us a list of sentences\n",
    "            print(sent_text)\n",
    "\t\tfor sentence in sent_text:\n",
    "\t\t\tself.inputLineCount = self.inputLineCount  + 1\n",
    "\n",
    "\t\t\ttokens = sentence.split()\n",
    "\t\t\tkeyList = [ ];\n",
    "\n",
    "\t\t\t#Add a special key with just beginning words\n",
    "\t\t\tself.table.setdefault( '#BEGIN#', []).append(tokens[0:self.order ]);\n",
    "\n",
    "\t\t\t#loop through each word, and if we have enough to add dictionary item, then add\n",
    "\t\t\tfor item in tokens:\n",
    "\t\t\t\t\n",
    "\t\t\t\t#item = self.__clean(item)\n",
    "\n",
    "\t\t\t\tif len(keyList) < self.order :  #not enough items\n",
    "\t\t\t\t\tkeyList.append(item)\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t#If we already have the item, then add it, otherwise add to empty list\n",
    "\t\t\t\tself.table.setdefault( tuple(keyList), []).append(item)\n",
    "\n",
    "\t\t\t\t#Remove the first word and push last word on to it\n",
    "\t\t\t\tkeyList.pop(0)\n",
    "\t\t\t\tkeyList.append(item)\n",
    "\t\t\t\tself.inputWordCount = self.inputWordCount + 1\n",
    "\n",
    "\t\t#print(self.table)\n",
    "\t\treturn\n",
    "\n",
    "\tdef setMaxWordInSentence(self, maxWordInSentence):\n",
    "\t\tself.maxWordInSentence = maxWordInSentence\n",
    "\n",
    "\tdef genText(self):\t\n",
    "\t\tkey = list(random.choice(self.table['#BEGIN#']))\n",
    "\n",
    "\t\tgenStr = \" \".join( key )\n",
    "\t\tfor _ in range( self.maxWordInSentence ):\n",
    "\t\t\t#print('---------------------')\n",
    "\t\t\t#print('key: ', key)\n",
    "\t\t\t#print(tuple(key))\n",
    "\t\t\tnewKey = self.table.setdefault( tuple(key), None) \n",
    "\t\t\tif(not newKey):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#print('new key: ', newKey)\n",
    "\t\t\tnewVal = random.choice( newKey )\n",
    "\t\t\t#print('new val: ', newVal)\n",
    "\t\t\tgenStr = genStr + \" \" + newVal\n",
    "\n",
    "\t\t\tkey.pop(0)\n",
    "\t\t\tkey.append(newVal)\n",
    "\n",
    "\t\treturn genStr\n",
    "\n",
    "\tdef getLineCount(self):\n",
    "\t\treturn self.inputLineCount\n",
    "\n",
    "\tdef getWordCount(self):\n",
    "\t\treturn self.inputWordCount\n",
    "\n",
    "\tdef outputDict(self, filename, fileEncoding=\"utf-8\"):\n",
    "\t\twith open(filename, 'w', encoding=fileEncoding) as file:\n",
    "\t\t\tpprint.pprint(self.table, file)\n",
    "\t\t#pprint.pprint(self.table,markovDictFile)\n",
    "\n",
    "\tdef __clean(self, text):\n",
    "\t\ttext = re.sub('<.+?>', '. ', text)\n",
    "\t\ttext = re.sub('&.+?;', '', text)\n",
    "\t\ttext = re.sub('[\\']{1}', '', text)\n",
    "\t\ttext = re.sub('[^a-zA-Z0-9\\s_\\-\\?:;\\.,!\\(\\)\\\"]+', ' ', text)\n",
    "\t\ttext = re.sub('\\s+', ' ', text)\n",
    "\t\ttext = re.sub('(\\.\\s*)+', '. ', text)\n",
    "\t\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "dirPath = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "dataPath = os.path.abspath(os.path.join(dirPath, '../../', 'data'))\n",
    "datasetPath =  os.path.abspath(os.path.join(dataPath, \"Newsroom/2_content.txt\"))\n",
    "outputPath = os.path.abspath(os.path.join(dirPath, '../../', 'output'))\n",
    "\n",
    "print(datasetPath)\n",
    "keyLen = 3\n",
    "fileList = []\n",
    "#dictionaryFilePath = os.path.abspath(os.path.join(outputPath, \"data_obama.txt\"))\n",
    "dictionaryFilePath = os.path.abspath(os.path.join(outputPath, \"newsroom.txt\"))\n",
    "print(dictionaryFilePath)\n",
    "if train :\n",
    "    fileList += glob.glob(datasetPath)\n",
    "    print(\"Input files\")\n",
    "    print(fileList)\n",
    "    markovObj = Markov(keyLen)\n",
    "\n",
    "\n",
    "    for file in fileList:\n",
    "        print(file)\n",
    "        try:\n",
    "            markovObj.readFile(file, \"utf-8\")\n",
    "        except:\n",
    "            markovObj.readFile(file, \"windows-1252\")\n",
    "\n",
    "    markovObj.outputDict(dictionaryFilePath)\n",
    "\n",
    "    print( \"Generated Markov dictionary %s with processing %s input lines and %s input words \" % ( dictionaryFilePath, str(markovObj.getLineCount()), str(markovObj.getWordCount()) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWordInSentence = 50\n",
    "genNSentences = 2\n",
    "outputText = []\n",
    "\n",
    "\n",
    "markovObj1 = Markov(dictFile=dictionaryFilePath, maxWordInSentence= maxWordInSentence)\n",
    "\n",
    "for _ in range( genNSentences ):\n",
    "    text = markovObj1.genText() \n",
    "    print( text )\n",
    "    if len(text) <= 140 and text.endswith('.'):\n",
    "        outputText.append(text)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(outputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
